# -*- coding: utf-8 -*-
"""NSFW Disabled: NOP's Stable Diffusion Colab v0.35 (1.4 Weights)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17

# NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.35 (1.4 Weights)

WAS will be developing this colab with me, he has been doing great work! The logical thing would be to include him here, throw him some kudos for the great work done when you see him! :)

Trying to make this a one-stop shop for various programs + a GOTO guide on how to install everything locally. If you have suggestions, bug reports, or implementations, feel free to contact me on Discord and/or leave a comment via colab's comment feature. NOP#1337


Changelog:
- v0.32: Added options for samplers (still having problems with other ones). Also added an option to sharpen the image.
- v0.33: Added ddim & ETA for DDIM. Also trying to dim down some more VRAM
- v0.34: Merged setup into the render cell.
- v0.35: Just some backend stuff. Will behave differently, don't be alarmed. Sometimes VRAM got stuck and this should fix it. Also added a 'SKIP_PREVIEW' button to see if this can fix connectivity issues after running the colab.



By NOP#1337 & WAS#0263

Diffusers updated this morning breaking a few things (and may have introduced a few bugs into the actual program). I think I got most of the issues resolved via the colab, but let me know if there are any other issues that I may have missed. Thank you ! Nee#9981 for bug hunting!

#Scheduler/Sampler Study

Kudos to scarletpenn#7121 !

![](https://cdn.discordapp.com/attachments/1002602742667280404/1014634578226462740/K-LMS_vs_PNDM_vs_DDIM_0-1.0.png)

##Changelog/Credits/FAQ/TODO

Changelog:

- v0.1: Colab creation
- v0.11: Google Drive option for TXT2IMG and some error corrections
- v0.12: Added more options to TXT2IMG
- v0.13: Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version
- v0.14: Added in full precision in the diffuser method
- v0.15: Added in file saving in drive for diffusers
- v0.16: Added in prompt saving
- v0.17: Added in the new weights and disabled the NSFW check
- v0.18: Minor adjustments and more details saved in prompt saving
- v0.19: Added in modifier experiments in Diffusers + example. More options to experiment will come with future updates
- v0.20: Low VRAM patch is fixed. Getting 10 it/s with it on with a V100
- v0.21: Diffusers now has an upscaler (Real-ESRGAN) <-- just updated to GFPGAN
- v0.22: Added in a small little fun randomizer
- v0.23: Now I support both upscalers. GFP is really good at faces but kind of sucks at upscaling. If you want the best of two worlds choose "Both" as an upscaler. T4 may have problems with one or both of them, looking at a fix for that (May get lucky with Real-ESRGAN).
- v0.30: A complete code overhaul by WAS#0263 and a bunch of stuff added. With an overhaul, there could also different bugs. Shoot me ( NOP#1337 ) a discord message when you find one and tell WAS that he is awesome when you see him! If there are major bugs, I'll fix them as soon as I can
- v0.31: Forgot to mention last update: No more huggingface login, that's all built-in now. Also, we have new facial enhancement. --> CodeFormer. It's like GFP but not as strong + with a nifty slider

Credits:
- WAS#0263 for giving great advice, coding tips, code, and recommendations. A MASSIVE help overhauling this thing
- ùìëùìµùì™ùì∑ùì¨ùììùìÆùìûùìØùìØùì≤ùì¨ùì≤ùì™ùìµ#2485 for inspiring me to put an upscaler in the colab and for bug hunting
- Gecktendo#8043 for helping with the default prompt
- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk

FAQ:

Q: What is the difference between Diffusers and TXT2IMG?

A: Diffusers is the Huggingface Python Library and TXT2IMG is from the Stability AI Github. They both do the same thing, but differently. Whichever you want to use is just personal preference.

Q: Which one should I use?

A: Really just personal preference. For me: Right now I am heavily concentrating on diffusers just because it's a tad easier to work with.

TODO:
- Saving weights in Google Drive
- Option to delete original images if upscaled
- Option to pick older models
- Add in the optimized Stable Diffusion fork ( https://github.com/basujindal/stable-diffusion )
- Implement https://github.com/DamascusGit/stable-diffusion/blob/main/scripts/txt2img_k.py
- Add offline install instructions (50% done)
- Add in k-diffusion
- Add in inpainting
- Add init functionality
- Add in a plms, ddim, etc choice (50% done)
- Option to load a config file to load in preset settings
- More functionality

##GPU Info
 (If it throws an error here, go to Runtime, then click "Change Runtime Type" and then select "GPU"). There's also a chance that Colab put you on a GPU timeout if this is set and it still throws an error
"""

!nvidia-smi

"""# Diffusers Method

Merged everything into one cell. Just click Render and you are good-to-go. First run will take a while since it has to setup everything, but afterwards it should be quick to render with every go-around (or at least until the session disconnects)

If anyone having problems running this colab on mobile device then try checking the `Desktop Site` in chrome from the menu from top right corner . (Kudos to Rohan Singh)

It will behave a little differently than you all are used to, don't be alarmed. Did some backend stuff to try and mitigate some vram issues. It will clear VRAM if you interrupt the cell or if you get a VRAM error. If that happens, it takes a little bit longer than usual to spin up. If you let the iterations finish then the next run-through should be quicker.
"""

# Commented out IPython magic to ensure Python compatibility.
try:
  import os, torch, gc
except ValueError:
  pass
  !pip uninstall numpy
  !pip install -U numpy
  import os, torch, gc

from PIL import Image
import random, time, shutil, sys
from contextlib import contextmanager, nullcontext
from torch import autocast
from IPython.display import clear_output
import numpy as np


#@title Render Images
PROMPT = "anime key visual of a woman, greg rutkowski, artgerm, mucha, trending on artstation, ethereal, misty, mysterious, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman" #@param {type:'string'}
PROMPT_FILE = '' #@param {type: 'string'}
#@markdown `PROMPT_FILE` is a text file that contains a prompt per line. 
STEPS = 160 #@param {type:"slider", min:5, max:500, step:5} 
SCHEDULER = 'default' #@param ["default", "pndm", "k-lms", "ddim"]
DDIM_ETA = 0.89 #@param {type:"slider", min:0, max:1, step:0.01} 
#@markdown Getting good results with `ddim` and `DDIM_ETA` at 0.9
#@markdown Diffusion steps determine the quality of the final image
SEED = 0 #@param {type:'integer'}
#@markdown The seed used for the generation. Leave at `0` for random.
NUM_ITERS = 8 #@param {type:"slider", min:1, max:100, step:1} 
RUN_FOREVER = False #@param {type:"boolean"}
#@markdown Number of iterations for a given prompt.
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.8 #@param {type:"slider", min:0, max:25, step:0.1}
#@markdown The CFG scale determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.<br>
PRECISION = "autocast" #@param ["full","autocast"]
#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>
SAVE_PROMPT_DETAILS = True #@param {type:"boolean"}
USE_DRIVE_FOR_PICS = True #@param {type:"boolean"}
DRIVE_PIC_DIR = "AI_PICS" #@param {type:"string"}
if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):
  from google.colab import drive
  drive.mount('/content/drive')
if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):
  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR
if USE_DRIVE_FOR_PICS:
  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'
else:
  OUTDIR = '/content/diffusers_output'
try:
  os.makedirs(OUTDIR)
except FileExistsError:
  pass
#@markdown ---
#@markdown `IMAGE_UPSCALER`: May not work with the Tesla T4. GFP is pretty good at faces and the enhanced Real-ESRGAN is a pretty good uspcaler. If both is selected, then GFPGAN will act as a face enhancer and Real-ESRGAN will act as the upscaler. Same applies with Codeformer, which seems to be a little more mild than GFP.<br>Recommendations: GFPGAN if you only have faces or People. ESRGAN if you have no people in your prompt. Both if you have people and other things in your prompt<br>Note: ESRGAN only support 2x, 4x, and 8x, if any other value is selected, it will pick the nearest value
IMAGE_UPSCALER = "CodeFormer + Enhanced ESRGAN" #@param ["None","GFPGAN","Enhanced Real-ESRGAN", "GFPGAN + Enhanced ESRGAN", "CodeFormer", "CodeFormer + Enhanced ESRGAN"]
UPSCALE_AMOUNT = 2 #@param {type:"raw"}
#@markdown `CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)<br>
CODEFORMER_FIDELITY = 0.5 #@param {type:"slider", min:0, max:1, step:0.01}
#@markdown `SHARPEN_AMOUNT`: Select 0 to turn it off
SHARPEN_AMOUNT = 2 #@param{type:'slider', min:0, max:3, step:1}
DELETE_ORIGINALS = False #@param{type:'boolean'}
#@markdown `SKIP_PREVIEW`: Clicking this might help with connection issues (especially on mobile). It will only show the original result, not the improvements
SKIP_PREVIEW = False #@param{type:'boolean'}
precision_scope = autocast if PRECISION=="autocast" else nullcontext
ORIG_SEED = SEED
DRIVE_PIC_DIR = DRIVE_PIC_DIR.strip()
# %cd /content/
#@markdown ---
#@markdown <b>SETUP</b><br>
#@markdown `LOW_VRAM_PATCH`: Click this if you have CUDA out of memory errors with low settings. If you check this you may be tied to using this setting until the next session restart since it patches various files. <br> This should also speed up iterations but could output lower quality content<br>
#@markdown `ENABLE_NSFW_FILTER`: Will ENABLE the NSFW filter. If you want uncensored results, do not click that. Might need a session restart if you have run this setup prior without it checked since it patches files.<br>
LOW_VRAM_PATCH = False #@param {type:"boolean"}
ENABLE_NSFW_FILTER = False #@param {type:"boolean"}
CLEAR_SETUP_LOG = True #@param{type: 'boolean'}
#markdown Clear the setup log after installation compeltes.
model_id = "CompVis/stable-diffusion-v1-4"

#@markdown ---
# Enable third-party widgets
from google.colab import output
output.enable_custom_widget_manager()

def clean_env():
  gc.collect()
  torch.cuda.empty_cache()

def fetch_bytes(url_or_path):
    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):
        from urllib.request import urlopen 
        return urlopen(url_or_path) 
    return open(url_or_path, 'r')

def patch_low_vram():
  patched_file = open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py').read().replace('input, self.num_groups, self.weight, self.bias, self.eps)','input, self.num_groups, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)')
  with open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py','w') as file:
    file.write(patched_file)
  with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py','w') as file:
    file.write(
      '''
import inspect
import warnings
from typing import List, Optional, Union

import torch

from tqdm.auto import tqdm
from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer

from ...models import AutoencoderKL, UNet2DConditionModel
from ...pipeline_utils import DiffusionPipeline
from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler
from .safety_checker import StableDiffusionSafetyChecker


class StableDiffusionPipeline(DiffusionPipeline):
  def __init__(
      self,
      vae: AutoencoderKL,
      text_encoder: CLIPTextModel,
      tokenizer: CLIPTokenizer,
      unet: UNet2DConditionModel,
      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],
      safety_checker: StableDiffusionSafetyChecker,
      feature_extractor: CLIPFeatureExtractor,
  ):
      super().__init__()
      scheduler = scheduler.set_format("pt")
      self.register_modules(
          vae=vae,
          text_encoder=text_encoder,
          tokenizer=tokenizer,
          unet=unet,
          scheduler=scheduler,
          safety_checker=safety_checker,
          feature_extractor=feature_extractor,
      )

  @torch.no_grad()
  def __call__(
      self,
      prompt: Union[str, List[str]],
      height: Optional[int] = 512,
      width: Optional[int] = 512,
      num_inference_steps: Optional[int] = 50,
      guidance_scale: Optional[float] = 7.5,
      eta: Optional[float] = 0.0,
      generator: Optional[torch.Generator] = None,
      output_type: Optional[str] = "pil",
      **kwargs,
  ):
      if "torch_device" in kwargs:
          device = kwargs.pop("torch_device")
          warnings.warn(
              "`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0."
              " Consider using `pipe.to(torch_device)` instead."
          )

          # Set device as before (to be removed in 0.3.0)
          if device is None:
              device = "cuda" if torch.cuda.is_available() else "cpu"
          self.to(device)

      if isinstance(prompt, str):
          batch_size = 1
      elif isinstance(prompt, list):
          batch_size = len(prompt)
      else:
          raise ValueError(f"`prompt` has to be of type `str` or `list` but is {type(prompt)}")

      if height % 8 != 0 or width % 8 != 0:
          raise ValueError(f"`height` and `width` have to be divisible by 8 but are {height} and {width}.")

      # get prompt text embeddings
      text_input = self.tokenizer(
          prompt,
          padding="max_length",
          max_length=self.tokenizer.model_max_length,
          truncation=True,
          return_tensors="pt",
      )
      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]

      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
      # corresponds to doing no classifier free guidance.
      do_classifier_free_guidance = guidance_scale > 1.0
      # get unconditional embeddings for classifier free guidance
      if do_classifier_free_guidance:
          max_length = text_input.input_ids.shape[-1]
          uncond_input = self.tokenizer(
              [""] * batch_size, padding="max_length", max_length=max_length, return_tensors="pt"
          )
          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]

          # For classifier free guidance, we need to do two forward passes.
          # Here we concatenate the unconditional and text embeddings into a single batch
          # to avoid doing two forward passes
          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

      # get the intial random noise
      latents = torch.randn(
          (batch_size, self.unet.in_channels, height // 8, width // 8),
          generator=generator,
          device=self.device,
      )
      latents = latents.half()

      # set timesteps
      accepts_offset = "offset" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())
      extra_set_kwargs = {}
      if accepts_offset:
          extra_set_kwargs["offset"] = 1

      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)

      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas
      if isinstance(self.scheduler, LMSDiscreteScheduler):
          latents = latents * self.scheduler.sigmas[0]

      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
      # eta (Œ∑) is only used with the DDIMScheduler, it will be ignored for other schedulers.
      # eta corresponds to Œ∑ in DDIM paper: https://arxiv.org/abs/2010.02502
      # and should be between [0, 1]
      accepts_eta = "eta" in set(inspect.signature(self.scheduler.step).parameters.keys())
      extra_step_kwargs = {}
      if accepts_eta:
          extra_step_kwargs["eta"] = eta

      for i, t in tqdm(enumerate(self.scheduler.timesteps)):
          # expand the latents if we are doing classifier free guidance
          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
          if isinstance(self.scheduler, LMSDiscreteScheduler):
              sigma = self.scheduler.sigmas[i]
              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)

          # predict the noise residual
          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)["sample"]

          # perform guidance
          if do_classifier_free_guidance:
              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

          # compute the previous noisy sample x_t -> x_t-1
          if isinstance(self.scheduler, LMSDiscreteScheduler):
              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)["prev_sample"]
          else:
              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)["prev_sample"]

      # scale and decode the image latents with vae
      latents = 1 / 0.18215 * latents
      image = self.vae.decode(latents)

      image = (image / 2 + 0.5).clamp(0, 1)
      image = image.cpu().permute(0, 2, 3, 1).numpy()

      # run safety checker
      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors="pt").to(self.device)
      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)

      if output_type == "pil":
          image = self.numpy_to_pil(image)

      return {"sample": image, "nsfw_content_detected": has_nsfw_concept}
      '''
        )

def patch_nsfw():
  with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:
        file.write('''
import numpy as np
import torch
import torch.nn as nn

from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel

from ...utils import logging


logger = logging.get_logger(__name__)


def cosine_distance(image_embeds, text_embeds):
    normalized_image_embeds = nn.functional.normalize(image_embeds)
    normalized_text_embeds = nn.functional.normalize(text_embeds)
    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)


class StableDiffusionSafetyChecker(PreTrainedModel):
    config_class = CLIPConfig

    def __init__(self, config: CLIPConfig):
        super().__init__(config)

        self.vision_model = CLIPVisionModel(config.vision_config)
        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)

        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)
        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)

        self.register_buffer("concept_embeds_weights", torch.ones(17))
        self.register_buffer("special_care_embeds_weights", torch.ones(3))

    @torch.no_grad()
    def forward(self, clip_input, images):
        pooled_output = self.vision_model(clip_input)[1]  # pooled_output
        image_embeds = self.visual_projection(pooled_output)

        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()
        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()

        result = []
        batch_size = image_embeds.shape[0]
        for i in range(batch_size):
            result_img = {"special_scores": {}, "special_care": [], "concept_scores": {}, "bad_concepts": []}

            # increase this value to create a stronger `nfsw` filter
            # at the cost of increasing the possibility of filtering benign images
            adjustment = 0.0

            for concet_idx in range(len(special_cos_dist[0])):
                concept_cos = special_cos_dist[i][concet_idx]
                concept_threshold = self.special_care_embeds_weights[concet_idx].item()
                result_img["special_scores"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)
                if result_img["special_scores"][concet_idx] > 0:
                    result_img["special_care"].append({concet_idx, result_img["special_scores"][concet_idx]})
                    adjustment = 0.01

            for concet_idx in range(len(cos_dist[0])):
                concept_cos = cos_dist[i][concet_idx]
                concept_threshold = self.concept_embeds_weights[concet_idx].item()
                result_img["concept_scores"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)
                if result_img["concept_scores"][concet_idx] > 0:
                    result_img["bad_concepts"].append(concet_idx)

            result.append(result_img)

        has_nsfw_concepts = [len(res["bad_concepts"]) > 0 for res in result]

        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):
        #    if has_nsfw_concept:
        #        images[idx] = np.zeros(images[idx].shape)  # black image

        if any(has_nsfw_concepts):
            logger.warning(
                "Potential NSFW content was detected in one or more images, but the NSFW filter is off."
            )

        return images, has_nsfw_concepts''')

def make_pipe():
  try:
    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler
    from diffusers import StableDiffusionPipeline
  except ModuleNotFoundError or ImportError:
    diffusers_install()
  global LOW_VRAM_PATCH
  global pipe
  if LOW_VRAM_PATCH:
      patch_low_vram()
      try:
        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision="fp16", torch_dtype=torch.float16, use_auth_token=True).to("cuda")
      except Exception:
        !pip install transformers
        try:
          with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:
            key = f.read().decode('utf-8').split(':')
        except OSError as e:
          print(e)
        huggingface_username = key[0].strip()
        huggingface_token = key[1].strip()
        !echo $huggingface_token | huggingface-cli login
        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision="fp16", torch_dtype=torch.float16, use_auth_token=True).to("cuda")
  else:
    try:
      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to("cuda")
    except Exception as e:
      print(e)
      !pip install transformers
      try:
        with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:
          key = f.read().decode('utf-8').split(':')
      except OSError as e:
        print(e)
      huggingface_username = key[0].strip()
      huggingface_token = key[1].strip()
      !echo $huggingface_token | huggingface-cli login
      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to("cuda")
  del pipe.vae.encoder

def make_scheduler():
  try:
    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler
  except ModuleNotFoundError or ImportError:
    diffusers_install()
    if SCHEDULER == 'default':
      pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000, skip_prk_steps=True)
    if SCHEDULER == 'pndm':
      pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000, skip_prk_steps=True)
    elif SCHEDULER == 'k-lms':
      pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)
    elif SCHEDULER == 'ddim':
      pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)

def make_image():
  gen_seed = torch.Generator("cuda").manual_seed(SEED)
  global pipe
  if SCHEDULER == 'ddim':
    try:
      image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)["sample"][0]
    except IndexError:
      try:
        image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)["sample"][0]
      except UnboundLocalError or NameError:
        make_pipe()
        make_scheduler()
        make_image()
  else:
    try:
      image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0]
    except IndexError:
      try:
        image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0]
      except UnboundLocalError or NameError:
        make_pipe()
        make_scheduler()
        make_image()
    except UnboundLocalError or NameError:
      make_pipe()
      make_scheduler()
      make_image
    except RuntimeError as e:
      print(e)
      print("Using more VRAM than is available. \nClearing up VRAM.\n The session will disconnect. Please run this cell again.\nIf it fails second run-through, then lower your settings and try again.")
      time.sleep(0.3)
      # pids = !sudo fuser -v /dev/nvidia* | awk $3
      # purged = []
      # pipe = None
      # for pid in list(pids)[-1].split(' '):
      #   if pid != '' and pid not in purged:
      #     purged.append(pid)
      #     !sudo kill -9 $pid
      # clean_env()
      os._exit(os.EX_OK)
  return image

def diffusers_install():
  try:
    with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:
      key = f.read().decode('utf-8').split(':')
  except OSError as e:
    print(e)
    
  huggingface_username = key[0].strip()
  huggingface_token = key[1].strip()
  

  try: 
    !git lfs install
    !GIT_LFS_SKIP_SMUDGE=0
    # This will take a while
    !git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4
    !pip install -U git+https://github.com/huggingface/diffusers.git
    # Disable NSFW check
    if not ENABLE_NSFW_FILTER:
      patch_nsfw()
    !pip install transformers
    # make sure you're logged in with `huggingface-cli login`
    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler

    !mkdir diffusers_output
    !pip install pytorch-pretrained-bert
    !pip install spacy ftfy
    !python -m spacy download en
    !pip install scipy
    !git clone --recursive https://github.com/crowsonkb/k-diffusion.git
    !echo $huggingface_token | huggingface-cli login
    make_pipe()
  except OSError as e:
    raise e
  except BaseException as e:
    raise e
  finally:
    if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()
    print("Setup complete.")

try:
  from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler
except ModuleNotFoundError:
  diffusers_install()
  from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler

def GFPGAN_install():
  if not os.path.exists('/content/GFPGAN'):
    !git clone https://github.com/TencentARC/GFPGAN.git
#     %cd GFPGAN
    # Set up the environment
    # Install basicsr - https://github.com/xinntao/BasicSR
    # We use BasicSR for both training and inference
    !pip install basicsr
    # Install facexlib - https://github.com/xinntao/facexlib
    # We use face detection and face restoration helper in the facexlib package
    !pip install facexlib
    # Install other depencencies
    !pip install -r requirements.txt
    !python setup.py develop
    !pip install realesrgan  # used for enhancing the background (non-face) regions
    # Download the pre-trained model
    # !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models
    # Now we use the V1.3 model for the demo
    !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models
#     %cd /content/
    clear_output()
    
def ESRGAN_install():
  if not os.path.exists('/content/Real-ESRGAN'):
    !git clone https://github.com/sberbank-ai/Real-ESRGAN
    !pip install -r Real-ESRGAN/requirements.txt
    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth
    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth
    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth
#   %cd Real-ESRGAN
  from realesrgan import RealESRGAN
  clear_output()

  device = torch.device('cuda')
  global UPSCALE_AMOUNT
  if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):
    def closest_value(input_list, input_value):
      difference = lambda input_list : abs(input_list - input_value)
      res = min(input_list, key=difference)
      return res
    nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)
    print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')
    UPSCALE_AMOUNT = nearest_value

  model = RealESRGAN(device, scale = UPSCALE_AMOUNT)
  model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')
#   %cd /content/

def CodeFormer_install():
  if not os.path.exists('/content/CodeFormer'):
#     %cd /content
    !git clone https://github.com/sczhou/CodeFormer.git
#     %cd CodeFormer
    !pip install -r requirements.txt
    # Install basicsr
    !python basicsr/setup.py develop

    # Download the pre-trained model
    !python scripts/download_pretrained_models.py facelib
    !python scripts/download_pretrained_models.py CodeFormer
    !mkdir temp
    !mkdir results
#     %cd /content/


if "GFPGAN" in IMAGE_UPSCALER:
  GFPGAN_install()
if "ESRGAN"in IMAGE_UPSCALER:
  ESRGAN_install()
if "CodeFormer" in IMAGE_UPSCALER:
  CodeFormer_install()

from PIL import Image
import random, time, shutil, sys
from contextlib import contextmanager, nullcontext
from torch import autocast
from IPython.display import clear_output
import numpy as np

# Diffuse Function
def upscale(image):
    try:
      from realesrgan import RealESRGAN
    except ModuleNotFoundError:
      if not os.path.exists('/content/Real-ESRGAN'):
        ESRGAN_install()
#         %cd /content/Real-ESRGAN
        from realesrgan import RealESRGAN
#         %cd /content
    device = torch.device('cuda')
    model = RealESRGAN(device, scale = UPSCALE_AMOUNT)
    try:
      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')
    except FileNotFoundError:
      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth
      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth
      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth
      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')
    sr_image = model.predict(np.array(image))
    return sr_image
def diffuse_run():
    global SEED
    global pipe
    import torch
    # Disable NSFW check
    if not ENABLE_NSFW_FILTER:
      patch_nsfw()
    if ORIG_SEED == 0:
      if iteration is 0:
        SEED = random.randint(0,sys.maxsize)
      if iteration is not 0:
        SEED += 1
    else:
      if iteration > 0:
        SEED += 1
    gen_seed = torch.Generator("cuda").manual_seed(SEED)
    epoch_time = int(time.time())
    print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')
    clean_env()
    try:
      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler
    except ModuleNotFoundError:
      diffusers_install()
      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler
    try:
      image = make_image()
    except NameError:
      make_pipe()
      make_scheduler()
      image = make_image()
    except RuntimeError as e:
      print(e)
      print("Using more VRAM than is available. \nClearing up VRAM.\n The session will disconnect. Please run this cell again.\nIf it fails second run-through, then lower your settings and try again.")
      time.sleep(0.3)
      # pids = !sudo fuser -v /dev/nvidia* | awk $3
      # purged = []
      # pipe = None
      # for pid in list(pids)[-1].split(' '):
      #   if pid != '' and pid not in purged:
      #     purged.append(pid)
      #     !sudo kill -9 $pid
      # clean_env()
      os._exit(os.EX_OK)
    display(image)
    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'
    filedir = f'{OUTDIR}/{filename}'
    image.save(filedir)
    clean_env()
    if IMAGE_UPSCALER == "GFPGAN":
      print('Face Enhancing and Upscaling... ')
#       %cd GFPGAN
      try:
        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s $UPSCALE_AMOUNT --bg_upsampler realesrgan
      except FileNotFoundError:
        ESRGAN_install()
      if not SKIP_PREVIEW:
        display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))
#       %cd ..
      print(f'Moving enhanced image to {OUTDIR}')
      filedir = f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'
      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', filedir)
      if DELETE_ORIGINALS:
        os.remove(f'{OUTDIR}/restored_imgs/{filename}')
      clean_env()
    if IMAGE_UPSCALER == "Enhanced Real-ESRGAN":
      print('Upscaling... ')
      sr_image = upscale(image)
      if not SKIP_PREVIEW:
        display(sr_image)
      old_filedir = filedir
      try:
        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png'
      except NameError:
        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png'
      sr_image.save(filedir)
      if DELETE_ORIGINALS:
        os.remove(old_filedir)
      clean_env()
    if IMAGE_UPSCALER == "GFPGAN + Enhanced ESRGAN":
      print('Face Enhancing... ')
#       %cd GFPGAN
      try:
        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s 1 --bg_upsampler realesrgan
      except FileNotFoundError:
        ESRGAN_install()
      if not SKIP_PREVIEW:
        display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))
#       %cd ..
      shutil.copy(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
      clean_env()
      print('Upscaling... ')
      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      if not SKIP_PREVIEW:
        display(sr_image)
      old_filedir = filedir
      try:
        filedir = f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'
      except NameError:
        filedir = f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'
      sr_image.save(filedir)
      if DELETE_ORIGINALS:
        try:
          os.remove(old_filedir)
        except Exception as e:
          print(e)
      clean_env()
    if IMAGE_UPSCALER == "CodeFormer":
      print("Face enhancing and Upscaling... ")
      # It was behaving weird, hence why I am doing this the weird way
      try:
        !cp $filedir /content/CodeFormer/temp/
      except Exception as e:
        os.makedirs('/content/CodeFormer/temp/')
        !cp $filedir /content/CodeFormer/temp/
#       %cd /content/CodeFormer/
      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale $UPSCALE_AMOUNT --bg_upsampler realesrgan
      old_filedir = filedir
      filedir = f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'
      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', filedir)
      os.remove(f'/content/CodeFormer/temp/{filename}')
      if DELETE_ORIGINALS:
        try:
          os.remove(old_filedir)
        except Exception as e:
          print(e)
#       %cd /content
      if not SKIP_PREVIEW:
        display(Image.open(filedir))
      clean_env()
    if IMAGE_UPSCALER == "CodeFormer + Enhanced ESRGAN":
      print("Face enhancing... ")
      try:
        !cp $filedir /content/CodeFormer/temp/
      except Exception as e:
        os.makedirs('/content/CodeFormer/temp/')
        !cp $filedir /content/CodeFormer/temp/
#       %cd /content/CodeFormer/
      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale 1 --bg_upsampler realesrgan
      os.remove(f'/content/CodeFormer/temp/{filename}')
      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png')
#       %cd /content
      if not SKIP_PREVIEW:
        display(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      clean_env()
      print("Upscaling... ")
      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'))
      if not SKIP_PREVIEW:
        display(sr_image)
      old_filedir = filedir
      filedir = f'{OUTDIR}/{filename.replace(".png","")}_upscaled_{UPSCALE_AMOUNT}.png'
      sr_image.save(filedir)
      if DELETE_ORIGINALS:
        try:
          os.remove(old_filedir)
        except Exception as e:
          print(e)
      clean_env()
    if int(SHARPEN_AMOUNT) != 0:
      def sharpenImage(image, samples=1):
        import PIL
        from PIL import Image, ImageFilter
        im = image
        for i in range(samples):
            im = im.filter(ImageFilter.SHARPEN)
        return im
      print(f"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\n")
      image = sharpenImage(Image.open(filedir), SHARPEN_AMOUNT)
      if not SKIP_PREVIEW:
        display(image)
      old_filedir = filedir
      filedir = f'{filedir.strip(".png")}_sharpened_{SHARPEN_AMOUNT}.png'
      image.save(filedir)
      if DELETE_ORIGINALS:
        try:
          os.remove(old_filedir)
        except Exception as e:
          print(e)
# End Diffuse Function


PROMPTS = []
if PROMPT_FILE not in ['','none']:
    try:
        with open(PROMPT_FILE, "r") as f:
            PROMPTS = f.read().splitlines()
    except OSError as e:
        raise e

if PROMPT not in ['', 'none']:
    PROMPTS.insert(0, PROMPT)

with torch.no_grad():
  with precision_scope("cuda"):
      if RUN_FOREVER:
        while True:
          for pi in PROMPTS:
            PROMPT = pi
            print(OUTDIR)
            if SAVE_PROMPT_DETAILS:
                epoch_time = int(time.time())
                try:
                  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:
                      file.write(f'{PROMPT}\n\nHeight: {HEIGHT}\nWidth: {WIDTH}\nSeed: {SEED}\nScale: {SCALE}\nPrecision: {PRECISION}\nETA:{DDIM_ETA}')
                except FileNotFoundError:
                  os.makedirs(OUTDIR)
            for iteration in range(NUM_ITERS):
              clean_env()
              try:
                diffuse_run()
              except KeyboardInterrupt:
                import os
                clean_env()
                os._exit(os.EX_OK)
              clean_env()
      else:
        for pi in PROMPTS:
            PROMPT = pi
            print(OUTDIR)
            if SAVE_PROMPT_DETAILS:
                epoch_time = int(time.time())
                try:
                  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:
                      file.write(f'{PROMPT}\n\nHeight: {HEIGHT}\nWidth: {WIDTH}\nSeed: {SEED}\nScale: {SCALE}\nPrecision: {PRECISION}\nETA:{DDIM_ETA}')
                except FileNotFoundError:
                  os.makedirs(OUTDIR)
            for iteration in range(NUM_ITERS):
              clean_env()
              try:
                diffuse_run()
              except KeyboardInterrupt:
                import os
                clean_env()
                os._exit(os.EX_OK)
              clean_env()
      clean_env()

"""## Diffuser Experiments (run through the Diffuser setup first)"""

#@title Modifier Tester
#@markdown `MODIFIER_FILE`: location of a text file with a list of modifiers seperated by newline.<br>You will need to upload it. Then right click on the file in colab and then click "copy path" (If you can't find it, click on the folder icon on the left pane). Then paste it in the box. See modifier_examples.txt for an example (if you're lazy, just edit the file. It will populate AFTER the first run)<br>
#@markdown `BASE_PROMPT`: the prompt against which the modifiers will be tested<br>
with open('modifier_examples.txt','w') as file:
  file.write('Canon\nNikon\nPanasonic\nSony\nDigital Painting\nMatte Painting\nDrawing from a 5 year old\nPasta Art\nI made this while on acid')

MODIFIER_FILE = "/content/modifier_examples.txt" #@param {type:'string'}
BASE_PROMPT = "A dog playing in a field" #@param {type:'string'}
STEPS = 50 #@param {type:"slider", min:5, max:500, step:5} 
SEED = 42 #@param {type:'integer'}
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.8 #@param {type:"slider", min:0, max:25, step:0.1}
PRECISION = "autocast" #@param ["full","autocast"]
SEED = torch.Generator("cuda").manual_seed(SEED)
import random
import torch
from contextlib import contextmanager, nullcontext
import time
import os
from torch import autocast

OUTDIR = '/content/experiments'
precision_scope = autocast if PRECISION=="autocast" else nullcontext
with open(MODIFIER_FILE) as file:
  for line in file.readlines():
    line = line.strip()
    PROMPT = f"{BASE_PROMPT}, {line}"
    print(f"Running: {PROMPT}")
    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=SEED)["sample"][0]  
    display(image)
    try:
      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')
    except FileNotFoundError:
      !mkdir $OUTDIR
      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')

#@title Randomizer, aka: I feel lucky/Fuck my shit up
import os
import random
WORDS_AMOUNT = 30 #@param {type:"integer"}
STEPS = 90 #@param {type:"slider", min:5, max:500, step:5}
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.7 #@param {type:"slider", min:0, max:25, step:0.1}
PRECISION = "autocast" #@param ["full","autocast"]
precision_scope = autocast if PRECISION=="autocast" else nullcontext

if not os.path.exists('words.txt'):
  !wget https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt -O words.txt
with open('words.txt') as file:
  words = file.readlines()
  prompt = ""
  for iteration in range(WORDS_AMOUNT):
    again = True
    while again:
      word = random.choice(words).strip()
      if not '#' in word:
        again = False
    prompt += f'{random.choice(words).strip()}, '
prompt = prompt[:-2]
print(f'Prompt: {prompt}')
with precision_scope("cuda"):
  seed = torch.Generator("cuda").manual_seed(random.randint(0,4294967295))
  image = pipe(prompt, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0] 
  display(image)

"""#TXT2IMG Method"""

#@title Huggingface Login
from getpass import getpass

huggingface_username = '' #@param {type:"string"}
huggingface_token = '' #@param {type:"string"}

# Commented out IPython magic to ensure Python compatibility.
#@title TXT2IMG Setup
import os
!pip install -q condacolab
import condacolab
import torch
torch.cuda.empty_cache()
condacolab.install()
root_code = root_model = "/content/stableai"
code_dir = root_code + "/stable-diffusion"
import os
if not os.path.isdir(root_code):
  !mkdir $root_code
# %cd $root_code
!git clone https://github.com/DamascusGit/stable-diffusion.git
!mamba env update -n base -f stable-diffusion/environment.yaml
!pip install torchmetrics==0.6.0
!pip install kornia==0.6)
if not os.path.isdir(root_model):
  !mkdir $root_model
# %cd $root_model
!git lfs install
!GIT_LFS_SKIP_SMUDGE=0
# Will take a long time
!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original
import numpy as np
import os
from PIL import Image
def display_last_grid(grid_dir):
  dir_list = os.listdir(grid_dir)
  dir_list.sort()
  #print (dir_list)
  last_image = dir_list[-2]
  img = Image.open(grid_dir + "/" + last_image).convert('RGB')
  target_size = 600
  img.thumbnail((target_size,target_size))
  display (img)
!mkdir /content/txt2img_output
# %cd $code_dir

# Commented out IPython magic to ensure Python compatibility.

import random
import torch
from contextlib import contextmanager, nullcontext
import time
import os
from torch import autocast
torch.cuda.empty_cache()
PROMPT = "matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars" #@param {type:'string'}
STEPS = 160 #@param {type:"slider", min:5, max:500, step:5} 
SEED = 0 #@param {type:'integer'}
NUM_ITERS = 6 #@param {type:"slider", min:1, max:100, step:1} 
WIDTH = 512 #@param {type:"slider", min:256, max:1920, step:64}
HEIGHT = 512 #@param {type:"slider", min:256, max:1920, step:64}
SCALE = 13.8 #@param {type:"slider", min:0, max:25, step:0.1}
PRECISION = "autocast" #@param ["full","autocast"]
SAVE_PROMPT_DETAILS = True #@param {type:"boolean"}
USE_DRIVE_FOR_PICS = True #@param {type:"boolean"}
DRIVE_PIC_DIR = "AI_PICS" #@param {type:"string"}
IMAGE_UPSCALER = True #@param {type:"boolean"}
UPSCALE_AMOUNT = "2" #@param ["2","4", "8"]
precision_scope = autocast if PRECISION=="autocast" else nullcontext
ORIG_SEED = SEED

# %cd /content/

if IMAGE_UPSCALER:
  def upscale(image):
    sr_image = model.predict(np.array(image))
    return sr_image
  if not os.path.exists('/content/Real-ESRGAN'):
    !git clone https://github.com/sberbank-ai/Real-ESRGAN
    !pip install -r Real-ESRGAN/requirements.txt
    !gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth
    !gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth
    !gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth
#   %cd /content/Real-ESRGAN
  from realesrgan import RealESRGAN
  from PIL import Image
  import numpy as np
  import torch

  device = torch.device('cuda')
  model = RealESRGAN(device, scale = int(UPSCALE_AMOUNT))
  model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')
#   %cd /content/

if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):
  from google.colab import drive
  drive.mount('/content/drive')
if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):
  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR
if USE_DRIVE_FOR_PICS:
  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'
else:
  OUTDIR = '/content/diffusers_output'
epoch_time = int(time.time())
if SAVE_PROMPT_DETAILS:
  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:
        file.write(f'{PROMPT}\n\nHeight: {HEIGHT}\nWidth: {WIDTH}\nSeed: {SEED}\nScale: {SCALE}\nPrecision: {PRECISION}\n')
with precision_scope("cuda"):
  for iteration in range(NUM_ITERS):
    
    if ORIG_SEED == 0:
      rand_num = random.randint(0,4294967295)
      gen_seed = torch.Generator("cuda").manual_seed(rand_num)
    else:
      gen_seed = torch.Generator("cuda").manual_seed(SEED)
    epoch_time = int(time.time())
    try:
      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')
    except NameError:
      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')
    
    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)["sample"][0]  
    display(image)
    try:
      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')
    except NameError:
      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')
    print('Upscaling... ')
    sr_image = upscale(image)
    display(sr_image)
    try:
      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')
    except NameError:
      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')
#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>
#@markdown `Out of Memory error`: If the VRAM stacks right after execution, sometimes it helps waiting for a minute before running it again. Looking at ways to force it to clear the VRAM

"""#How to install offline

https://rentry.org/SDInstallation
"""