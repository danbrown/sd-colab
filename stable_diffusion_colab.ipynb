{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.35 (1.4 Weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WAS will be developing this colab with me, he has been doing great work! The logical thing would be to include him here, throw him some kudos for the great work done when you see him! :)\n",
        "\n",
        "Trying to make this a one-stop shop for various programs + a GOTO guide on how to install everything locally. If you have suggestions, bug reports, or implementations, feel free to contact me on Discord and/or leave a comment via colab's comment feature. NOP#1337\n",
        "\n",
        "\n",
        "Changelog:\n",
        "- v0.32: Added options for samplers (still having problems with other ones). Also added an option to sharpen the image.\n",
        "- v0.33: Added ddim & ETA for DDIM. Also trying to dim down some more VRAM\n",
        "- v0.34: Merged setup into the render cell.\n",
        "- v0.35: Just some backend stuff. Will behave differently, don't be alarmed. Sometimes VRAM got stuck and this should fix it. Also added a 'SKIP_PREVIEW' button to see if this can fix connectivity issues after running the colab.\n",
        "\n",
        "\n",
        "\n",
        "By NOP#1337 & WAS#0263\n",
        "\n",
        "Diffusers updated this morning breaking a few things (and may have introduced a few bugs into the actual program). I think I got most of the issues resolved via the colab, but let me know if there are any other issues that I may have missed. Thank you ! Nee#9981 for bug hunting!"
      ],
      "metadata": {
        "id": "_Y6RXjS1tTji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scheduler/Sampler Study"
      ],
      "metadata": {
        "id": "TeIWggi6TGH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kudos to scarletpenn#7121 !\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/1002602742667280404/1014634578226462740/K-LMS_vs_PNDM_vs_DDIM_0-1.0.png)"
      ],
      "metadata": {
        "id": "tYJ8pdUoTilg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Changelog/Credits/FAQ/TODO"
      ],
      "metadata": {
        "id": "OPyJJ2z-RJB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changelog:\n",
        "\n",
        "- v0.1: Colab creation\n",
        "- v0.11: Google Drive option for TXT2IMG and some error corrections\n",
        "- v0.12: Added more options to TXT2IMG\n",
        "- v0.13: Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version\n",
        "- v0.14: Added in full precision in the diffuser method\n",
        "- v0.15: Added in file saving in drive for diffusers\n",
        "- v0.16: Added in prompt saving\n",
        "- v0.17: Added in the new weights and disabled the NSFW check\n",
        "- v0.18: Minor adjustments and more details saved in prompt saving\n",
        "- v0.19: Added in modifier experiments in Diffusers + example. More options to experiment will come with future updates\n",
        "- v0.20: Low VRAM patch is fixed. Getting 10 it/s with it on with a V100\n",
        "- v0.21: Diffusers now has an upscaler (Real-ESRGAN) <-- just updated to GFPGAN\n",
        "- v0.22: Added in a small little fun randomizer\n",
        "- v0.23: Now I support both upscalers. GFP is really good at faces but kind of sucks at upscaling. If you want the best of two worlds choose \"Both\" as an upscaler. T4 may have problems with one or both of them, looking at a fix for that (May get lucky with Real-ESRGAN).\n",
        "- v0.30: A complete code overhaul by WAS#0263 and a bunch of stuff added. With an overhaul, there could also different bugs. Shoot me ( NOP#1337 ) a discord message when you find one and tell WAS that he is awesome when you see him! If there are major bugs, I'll fix them as soon as I can\n",
        "- v0.31: Forgot to mention last update: No more huggingface login, that's all built-in now. Also, we have new facial enhancement. --> CodeFormer. It's like GFP but not as strong + with a nifty slider\n",
        "\n",
        "Credits:\n",
        "- WAS#0263 for giving great advice, coding tips, code, and recommendations. A MASSIVE help overhauling this thing\n",
        "- ùìëùìµùì™ùì∑ùì¨ùììùìÆùìûùìØùìØùì≤ùì¨ùì≤ùì™ùìµ#2485 for inspiring me to put an upscaler in the colab and for bug hunting\n",
        "- Gecktendo#8043 for helping with the default prompt\n",
        "- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk\n",
        "\n",
        "FAQ:\n",
        "\n",
        "Q: What is the difference between Diffusers and TXT2IMG?\n",
        "\n",
        "A: Diffusers is the Huggingface Python Library and TXT2IMG is from the Stability AI Github. They both do the same thing, but differently. Whichever you want to use is just personal preference.\n",
        "\n",
        "Q: Which one should I use?\n",
        "\n",
        "A: Really just personal preference. For me: Right now I am heavily concentrating on diffusers just because it's a tad easier to work with.\n",
        "\n",
        "TODO:\n",
        "- Saving weights in Google Drive\n",
        "- Option to delete original images if upscaled\n",
        "- Option to pick older models\n",
        "- Add in the optimized Stable Diffusion fork ( https://github.com/basujindal/stable-diffusion )\n",
        "- Implement https://github.com/DamascusGit/stable-diffusion/blob/main/scripts/txt2img_k.py\n",
        "- Add offline install instructions (50% done)\n",
        "- Add in k-diffusion\n",
        "- Add in inpainting\n",
        "- Add init functionality\n",
        "- Add in a plms, ddim, etc choice (50% done)\n",
        "- Option to load a config file to load in preset settings\n",
        "- More functionality"
      ],
      "metadata": {
        "id": "G2uscKB2ROiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPU Info\n",
        " (If it throws an error here, go to Runtime, then click \"Change Runtime Type\" and then select \"GPU\"). There's also a chance that Colab put you on a GPU timeout if this is set and it still throws an error"
      ],
      "metadata": {
        "id": "o5V9MWbFtpNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "_ekR-LW6trWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusers Method\n"
      ],
      "metadata": {
        "id": "VHaZZ0uKti1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merged everything into one cell. Just click Render and you are good-to-go. First run will take a while since it has to setup everything, but afterwards it should be quick to render with every go-around (or at least until the session disconnects)\n",
        "\n",
        "If anyone having problems running this colab on mobile device then try checking the `Desktop Site` in chrome from the menu from top right corner . (Kudos to Rohan Singh)\n",
        "\n",
        "It will behave a little differently than you all are used to, don't be alarmed. Did some backend stuff to try and mitigate some vram issues. It will clear VRAM if you interrupt the cell or if you get a VRAM error. If that happens, it takes a little bit longer than usual to spin up. If you let the iterations finish then the next run-through should be quicker."
      ],
      "metadata": {
        "id": "NUlgKvW_EfqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import os, torch, gc\n",
        "except ValueError:\n",
        "  pass\n",
        "  !pip uninstall numpy\n",
        "  !pip install -U numpy\n",
        "  import os, torch, gc\n",
        "\n",
        "from PIL import Image\n",
        "import random, time, shutil, sys\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from torch import autocast\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#@title Render Images\n",
        "PROMPT = \"anime key visual of a woman, greg rutkowski, artgerm, mucha, trending on artstation, ethereal, misty, mysterious, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:'string'}\n",
        "PROMPT_FILE = '' #@param {type: 'string'}\n",
        "#@markdown `PROMPT_FILE` is a text file that contains a prompt per line. \n",
        "STEPS = 160 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SCHEDULER = 'default' #@param [\"default\", \"pndm\", \"k-lms\", \"ddim\"]\n",
        "DDIM_ETA = 0.89 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "#@markdown Getting good results with `ddim` and `DDIM_ETA` at 0.9\n",
        "#@markdown Diffusion steps determine the quality of the final image\n",
        "SEED = 0 #@param {type:'integer'}\n",
        "#@markdown The seed used for the generation. Leave at `0` for random.\n",
        "NUM_ITERS = 8 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "RUN_FOREVER = False #@param {type:\"boolean\"}\n",
        "#@markdown Number of iterations for a given prompt.\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "#@markdown The CFG scale determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.<br>\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/diffusers_output'\n",
        "try:\n",
        "  os.makedirs(OUTDIR)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "#@markdown ---\n",
        "#@markdown `IMAGE_UPSCALER`: May not work with the Tesla T4. GFP is pretty good at faces and the enhanced Real-ESRGAN is a pretty good uspcaler. If both is selected, then GFPGAN will act as a face enhancer and Real-ESRGAN will act as the upscaler. Same applies with Codeformer, which seems to be a little more mild than GFP.<br>Recommendations: GFPGAN if you only have faces or People. ESRGAN if you have no people in your prompt. Both if you have people and other things in your prompt<br>Note: ESRGAN only support 2x, 4x, and 8x, if any other value is selected, it will pick the nearest value\n",
        "IMAGE_UPSCALER = \"CodeFormer + Enhanced ESRGAN\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "UPSCALE_AMOUNT = 2 #@param {type:\"raw\"}\n",
        "#@markdown `CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "CODEFORMER_FIDELITY = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown `SHARPEN_AMOUNT`: Select 0 to turn it off\n",
        "SHARPEN_AMOUNT = 2 #@param{type:'slider', min:0, max:3, step:1}\n",
        "DELETE_ORIGINALS = False #@param{type:'boolean'}\n",
        "#@markdown `SKIP_PREVIEW`: Clicking this might help with connection issues (especially on mobile). It will only show the original result, not the improvements\n",
        "SKIP_PREVIEW = False #@param{type:'boolean'}\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "DRIVE_PIC_DIR = DRIVE_PIC_DIR.strip()\n",
        "%cd /content/\n",
        "#@markdown ---\n",
        "#@markdown <b>SETUP</b><br>\n",
        "#@markdown `LOW_VRAM_PATCH`: Click this if you have CUDA out of memory errors with low settings. If you check this you may be tied to using this setting until the next session restart since it patches various files. <br> This should also speed up iterations but could output lower quality content<br>\n",
        "#@markdown `ENABLE_NSFW_FILTER`: Will ENABLE the NSFW filter. If you want uncensored results, do not click that. Might need a session restart if you have run this setup prior without it checked since it patches files.<br>\n",
        "LOW_VRAM_PATCH = False #@param {type:\"boolean\"}\n",
        "ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "#markdown Clear the setup log after installation compeltes.\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "#@markdown ---\n",
        "# Enable third-party widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "def clean_env():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "def patch_low_vram():\n",
        "  patched_file = open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py').read().replace('input, self.num_groups, self.weight, self.bias, self.eps)','input, self.num_groups, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)')\n",
        "  with open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py','w') as file:\n",
        "    file.write(patched_file)\n",
        "  with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py','w') as file:\n",
        "    file.write(\n",
        "      '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (Œ∑) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to Œ∑ in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "      '''\n",
        "        )\n",
        "\n",
        "def patch_nsfw():\n",
        "  with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:\n",
        "        file.write('''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n",
        "\n",
        "from ...utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def cosine_distance(image_embeds, text_embeds):\n",
        "    normalized_image_embeds = nn.functional.normalize(image_embeds)\n",
        "    normalized_text_embeds = nn.functional.normalize(text_embeds)\n",
        "    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)\n",
        "\n",
        "\n",
        "class StableDiffusionSafetyChecker(PreTrainedModel):\n",
        "    config_class = CLIPConfig\n",
        "\n",
        "    def __init__(self, config: CLIPConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
        "        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n",
        "        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n",
        "\n",
        "        self.register_buffer(\"concept_embeds_weights\", torch.ones(17))\n",
        "        self.register_buffer(\"special_care_embeds_weights\", torch.ones(3))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, clip_input, images):\n",
        "        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n",
        "        image_embeds = self.visual_projection(pooled_output)\n",
        "\n",
        "        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()\n",
        "        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()\n",
        "\n",
        "        result = []\n",
        "        batch_size = image_embeds.shape[0]\n",
        "        for i in range(batch_size):\n",
        "            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n",
        "\n",
        "            # increase this value to create a stronger `nfsw` filter\n",
        "            # at the cost of increasing the possibility of filtering benign images\n",
        "            adjustment = 0.0\n",
        "\n",
        "            for concet_idx in range(len(special_cos_dist[0])):\n",
        "                concept_cos = special_cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.special_care_embeds_weights[concet_idx].item()\n",
        "                result_img[\"special_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"special_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"special_care\"].append({concet_idx, result_img[\"special_scores\"][concet_idx]})\n",
        "                    adjustment = 0.01\n",
        "\n",
        "            for concet_idx in range(len(cos_dist[0])):\n",
        "                concept_cos = cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.concept_embeds_weights[concet_idx].item()\n",
        "                result_img[\"concept_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"concept_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"bad_concepts\"].append(concet_idx)\n",
        "\n",
        "            result.append(result_img)\n",
        "\n",
        "        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n",
        "\n",
        "        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n",
        "        #    if has_nsfw_concept:\n",
        "        #        images[idx] = np.zeros(images[idx].shape)  # black image\n",
        "\n",
        "        if any(has_nsfw_concepts):\n",
        "            logger.warning(\n",
        "                \"Potential NSFW content was detected in one or more images, but the NSFW filter is off.\"\n",
        "            )\n",
        "\n",
        "        return images, has_nsfw_concepts''')\n",
        "\n",
        "def make_pipe():\n",
        "  try:\n",
        "    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "  except ModuleNotFoundError or ImportError:\n",
        "    diffusers_install()\n",
        "  global LOW_VRAM_PATCH\n",
        "  global pipe\n",
        "  if LOW_VRAM_PATCH:\n",
        "      patch_low_vram()\n",
        "      try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        try:\n",
        "          with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "            key = f.read().decode('utf-8').split(':')\n",
        "        except OSError as e:\n",
        "          print(e)\n",
        "        huggingface_username = key[0].strip()\n",
        "        huggingface_token = key[1].strip()\n",
        "        !echo $huggingface_token | huggingface-cli login\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "  else:\n",
        "    try:\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      !pip install transformers\n",
        "      try:\n",
        "        with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "          key = f.read().decode('utf-8').split(':')\n",
        "      except OSError as e:\n",
        "        print(e)\n",
        "      huggingface_username = key[0].strip()\n",
        "      huggingface_token = key[1].strip()\n",
        "      !echo $huggingface_token | huggingface-cli login\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "  del pipe.vae.encoder\n",
        "\n",
        "def make_scheduler():\n",
        "  try:\n",
        "    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "  except ModuleNotFoundError or ImportError:\n",
        "    diffusers_install()\n",
        "    if SCHEDULER == 'default':\n",
        "      pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "    if SCHEDULER == 'pndm':\n",
        "      pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "    elif SCHEDULER == 'k-lms':\n",
        "      pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "    elif SCHEDULER == 'ddim':\n",
        "      pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "def make_image():\n",
        "  gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "  global pipe\n",
        "  if SCHEDULER == 'ddim':\n",
        "    try:\n",
        "      image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "    except IndexError:\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except UnboundLocalError or NameError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image()\n",
        "  else:\n",
        "    try:\n",
        "      image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "    except IndexError:\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except UnboundLocalError or NameError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image()\n",
        "    except UnboundLocalError or NameError:\n",
        "      make_pipe()\n",
        "      make_scheduler()\n",
        "      make_image\n",
        "    except RuntimeError as e:\n",
        "      print(e)\n",
        "      print(\"Using more VRAM than is available. \\nClearing up VRAM.\\n The session will disconnect. Please run this cell again.\\nIf it fails second run-through, then lower your settings and try again.\")\n",
        "      time.sleep(0.3)\n",
        "      # pids = !sudo fuser -v /dev/nvidia* | awk $3\n",
        "      # purged = []\n",
        "      # pipe = None\n",
        "      # for pid in list(pids)[-1].split(' '):\n",
        "      #   if pid != '' and pid not in purged:\n",
        "      #     purged.append(pid)\n",
        "      #     !sudo kill -9 $pid\n",
        "      # clean_env()\n",
        "      os._exit(os.EX_OK)\n",
        "  return image\n",
        "\n",
        "def diffusers_install():\n",
        "  try:\n",
        "    with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "      key = f.read().decode('utf-8').split(':')\n",
        "  except OSError as e:\n",
        "    print(e)\n",
        "    \n",
        "  huggingface_username = key[0].strip()\n",
        "  huggingface_token = key[1].strip()\n",
        "  \n",
        "\n",
        "  try: \n",
        "    !git lfs install\n",
        "    !GIT_LFS_SKIP_SMUDGE=0\n",
        "    # This will take a while\n",
        "    !git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "    !pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "    # Disable NSFW check\n",
        "    if not ENABLE_NSFW_FILTER:\n",
        "      patch_nsfw()\n",
        "    !pip install transformers\n",
        "    # make sure you're logged in with `huggingface-cli login`\n",
        "    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "\n",
        "    !mkdir diffusers_output\n",
        "    !pip install pytorch-pretrained-bert\n",
        "    !pip install spacy ftfy\n",
        "    !python -m spacy download en\n",
        "    !pip install scipy\n",
        "    !git clone --recursive https://github.com/crowsonkb/k-diffusion.git\n",
        "    !echo $huggingface_token | huggingface-cli login\n",
        "    make_pipe()\n",
        "  except OSError as e:\n",
        "    raise e\n",
        "  except BaseException as e:\n",
        "    raise e\n",
        "  finally:\n",
        "    if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "    print(\"Setup complete.\")\n",
        "\n",
        "try:\n",
        "  from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "except ModuleNotFoundError:\n",
        "  diffusers_install()\n",
        "  from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "\n",
        "def GFPGAN_install():\n",
        "  if not os.path.exists('/content/GFPGAN'):\n",
        "    !git clone https://github.com/TencentARC/GFPGAN.git\n",
        "    %cd GFPGAN\n",
        "    # Set up the environment\n",
        "    # Install basicsr - https://github.com/xinntao/BasicSR\n",
        "    # We use BasicSR for both training and inference\n",
        "    !pip install basicsr\n",
        "    # Install facexlib - https://github.com/xinntao/facexlib\n",
        "    # We use face detection and face restoration helper in the facexlib package\n",
        "    !pip install facexlib\n",
        "    # Install other depencencies\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    !pip install realesrgan  # used for enhancing the background (non-face) regions\n",
        "    # Download the pre-trained model\n",
        "    # !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models\n",
        "    # Now we use the V1.3 model for the demo\n",
        "    !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "    %cd /content/\n",
        "    clear_output()\n",
        "    \n",
        "def ESRGAN_install():\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  clear_output()\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  global UPSCALE_AMOUNT\n",
        "  if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "    def closest_value(input_list, input_value):\n",
        "      difference = lambda input_list : abs(input_list - input_value)\n",
        "      res = min(input_list, key=difference)\n",
        "      return res\n",
        "    nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "    print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "    UPSCALE_AMOUNT = nearest_value\n",
        "\n",
        "  model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "  model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "\n",
        "def CodeFormer_install():\n",
        "  if not os.path.exists('/content/CodeFormer'):\n",
        "    %cd /content\n",
        "    !git clone https://github.com/sczhou/CodeFormer.git\n",
        "    %cd CodeFormer\n",
        "    !pip install -r requirements.txt\n",
        "    # Install basicsr\n",
        "    !python basicsr/setup.py develop\n",
        "\n",
        "    # Download the pre-trained model\n",
        "    !python scripts/download_pretrained_models.py facelib\n",
        "    !python scripts/download_pretrained_models.py CodeFormer\n",
        "    !mkdir temp\n",
        "    !mkdir results\n",
        "    %cd /content/\n",
        "\n",
        "\n",
        "if \"GFPGAN\" in IMAGE_UPSCALER:\n",
        "  GFPGAN_install()\n",
        "if \"ESRGAN\"in IMAGE_UPSCALER:\n",
        "  ESRGAN_install()\n",
        "if \"CodeFormer\" in IMAGE_UPSCALER:\n",
        "  CodeFormer_install()\n",
        "\n",
        "from PIL import Image\n",
        "import random, time, shutil, sys\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from torch import autocast\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "# Diffuse Function\n",
        "def upscale(image):\n",
        "    try:\n",
        "      from realesrgan import RealESRGAN\n",
        "    except ModuleNotFoundError:\n",
        "      if not os.path.exists('/content/Real-ESRGAN'):\n",
        "        ESRGAN_install()\n",
        "        %cd /content/Real-ESRGAN\n",
        "        from realesrgan import RealESRGAN\n",
        "        %cd /content\n",
        "    device = torch.device('cuda')\n",
        "    model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "    try:\n",
        "      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    except FileNotFoundError:\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "def diffuse_run():\n",
        "    global SEED\n",
        "    global pipe\n",
        "    import torch\n",
        "    # Disable NSFW check\n",
        "    if not ENABLE_NSFW_FILTER:\n",
        "      patch_nsfw()\n",
        "    if ORIG_SEED == 0:\n",
        "      if iteration is 0:\n",
        "        SEED = random.randint(0,sys.maxsize)\n",
        "      if iteration is not 0:\n",
        "        SEED += 1\n",
        "    else:\n",
        "      if iteration > 0:\n",
        "        SEED += 1\n",
        "    gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    clean_env()\n",
        "    try:\n",
        "      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    except ModuleNotFoundError:\n",
        "      diffusers_install()\n",
        "      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    try:\n",
        "      image = make_image()\n",
        "    except NameError:\n",
        "      make_pipe()\n",
        "      make_scheduler()\n",
        "      image = make_image()\n",
        "    except RuntimeError as e:\n",
        "      print(e)\n",
        "      print(\"Using more VRAM than is available. \\nClearing up VRAM.\\n The session will disconnect. Please run this cell again.\\nIf it fails second run-through, then lower your settings and try again.\")\n",
        "      time.sleep(0.3)\n",
        "      # pids = !sudo fuser -v /dev/nvidia* | awk $3\n",
        "      # purged = []\n",
        "      # pipe = None\n",
        "      # for pid in list(pids)[-1].split(' '):\n",
        "      #   if pid != '' and pid not in purged:\n",
        "      #     purged.append(pid)\n",
        "      #     !sudo kill -9 $pid\n",
        "      # clean_env()\n",
        "      os._exit(os.EX_OK)\n",
        "    display(image)\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "    clean_env()\n",
        "    if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "      print('Face Enhancing and Upscaling... ')\n",
        "      %cd GFPGAN\n",
        "      try:\n",
        "        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      except FileNotFoundError:\n",
        "        ESRGAN_install()\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      print(f'Moving enhanced image to {OUTDIR}')\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        os.remove(f'{OUTDIR}/restored_imgs/{filename}')\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(image)\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      try:\n",
        "        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      except NameError:\n",
        "        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        os.remove(old_filedir)\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "      print('Face Enhancing... ')\n",
        "      %cd GFPGAN\n",
        "      try:\n",
        "        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s 1 --bg_upsampler realesrgan\n",
        "      except FileNotFoundError:\n",
        "        ESRGAN_install()\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      shutil.copy(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      clean_env()\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      try:\n",
        "        filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      except NameError:\n",
        "        filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "      print(\"Face enhancing and Upscaling... \")\n",
        "      # It was behaving weird, hence why I am doing this the weird way\n",
        "      try:\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      except Exception as e:\n",
        "        os.makedirs('/content/CodeFormer/temp/')\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      %cd /content/CodeFormer/\n",
        "      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', filedir)\n",
        "      os.remove(f'/content/CodeFormer/temp/{filename}')\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      %cd /content\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(Image.open(filedir))\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "      print(\"Face enhancing... \")\n",
        "      try:\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      except Exception as e:\n",
        "        os.makedirs('/content/CodeFormer/temp/')\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      %cd /content/CodeFormer/\n",
        "      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale 1 --bg_upsampler realesrgan\n",
        "      os.remove(f'/content/CodeFormer/temp/{filename}')\n",
        "      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      %cd /content\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      clean_env()\n",
        "      print(\"Upscaling... \")\n",
        "      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      clean_env()\n",
        "    if int(SHARPEN_AMOUNT) != 0:\n",
        "      def sharpenImage(image, samples=1):\n",
        "        import PIL\n",
        "        from PIL import Image, ImageFilter\n",
        "        im = image\n",
        "        for i in range(samples):\n",
        "            im = im.filter(ImageFilter.SHARPEN)\n",
        "        return im\n",
        "      print(f\"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\\n\")\n",
        "      image = sharpenImage(Image.open(filedir), SHARPEN_AMOUNT)\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(image)\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{filedir.strip(\".png\")}_sharpened_{SHARPEN_AMOUNT}.png'\n",
        "      image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "# End Diffuse Function\n",
        "\n",
        "\n",
        "PROMPTS = []\n",
        "if PROMPT_FILE not in ['','none']:\n",
        "    try:\n",
        "        with open(PROMPT_FILE, \"r\") as f:\n",
        "            PROMPTS = f.read().splitlines()\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "\n",
        "if PROMPT not in ['', 'none']:\n",
        "    PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "with torch.no_grad():\n",
        "  with precision_scope(\"cuda\"):\n",
        "      if RUN_FOREVER:\n",
        "        while True:\n",
        "          for pi in PROMPTS:\n",
        "            PROMPT = pi\n",
        "            print(OUTDIR)\n",
        "            if SAVE_PROMPT_DETAILS:\n",
        "                epoch_time = int(time.time())\n",
        "                try:\n",
        "                  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                      file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\nETA:{DDIM_ETA}')\n",
        "                except FileNotFoundError:\n",
        "                  os.makedirs(OUTDIR)\n",
        "            for iteration in range(NUM_ITERS):\n",
        "              clean_env()\n",
        "              try:\n",
        "                diffuse_run()\n",
        "              except KeyboardInterrupt:\n",
        "                import os\n",
        "                clean_env()\n",
        "                os._exit(os.EX_OK)\n",
        "              clean_env()\n",
        "      else:\n",
        "        for pi in PROMPTS:\n",
        "            PROMPT = pi\n",
        "            print(OUTDIR)\n",
        "            if SAVE_PROMPT_DETAILS:\n",
        "                epoch_time = int(time.time())\n",
        "                try:\n",
        "                  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                      file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\nETA:{DDIM_ETA}')\n",
        "                except FileNotFoundError:\n",
        "                  os.makedirs(OUTDIR)\n",
        "            for iteration in range(NUM_ITERS):\n",
        "              clean_env()\n",
        "              try:\n",
        "                diffuse_run()\n",
        "              except KeyboardInterrupt:\n",
        "                import os\n",
        "                clean_env()\n",
        "                os._exit(os.EX_OK)\n",
        "              clean_env()\n",
        "      clean_env()"
      ],
      "metadata": {
        "id": "Ucr5_i21xSjv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffuser Experiments (run through the Diffuser setup first)"
      ],
      "metadata": {
        "id": "Z-cd15ZYjLqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modifier Tester\n",
        "#@markdown `MODIFIER_FILE`: location of a text file with a list of modifiers seperated by newline.<br>You will need to upload it. Then right click on the file in colab and then click \"copy path\" (If you can't find it, click on the folder icon on the left pane). Then paste it in the box. See modifier_examples.txt for an example (if you're lazy, just edit the file. It will populate AFTER the first run)<br>\n",
        "#@markdown `BASE_PROMPT`: the prompt against which the modifiers will be tested<br>\n",
        "with open('modifier_examples.txt','w') as file:\n",
        "  file.write('Canon\\nNikon\\nPanasonic\\nSony\\nDigital Painting\\nMatte Painting\\nDrawing from a 5 year old\\nPasta Art\\nI made this while on acid')\n",
        "\n",
        "MODIFIER_FILE = \"/content/modifier_examples.txt\" #@param {type:'string'}\n",
        "BASE_PROMPT = \"A dog playing in a field\" #@param {type:'string'}\n",
        "STEPS = 50 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 42 #@param {type:'integer'}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SEED = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "\n",
        "OUTDIR = '/content/experiments'\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "with open(MODIFIER_FILE) as file:\n",
        "  for line in file.readlines():\n",
        "    line = line.strip()\n",
        "    PROMPT = f\"{BASE_PROMPT}, {line}\"\n",
        "    print(f\"Running: {PROMPT}\")\n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=SEED)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    except FileNotFoundError:\n",
        "      !mkdir $OUTDIR\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jdSQSFlRjONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Randomizer, aka: I feel lucky/Fuck my shit up\n",
        "import os\n",
        "import random\n",
        "WORDS_AMOUNT = 30 #@param {type:\"integer\"}\n",
        "STEPS = 90 #@param {type:\"slider\", min:5, max:500, step:5}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.7 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "\n",
        "if not os.path.exists('words.txt'):\n",
        "  !wget https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt -O words.txt\n",
        "with open('words.txt') as file:\n",
        "  words = file.readlines()\n",
        "  prompt = \"\"\n",
        "  for iteration in range(WORDS_AMOUNT):\n",
        "    again = True\n",
        "    while again:\n",
        "      word = random.choice(words).strip()\n",
        "      if not '#' in word:\n",
        "        again = False\n",
        "    prompt += f'{random.choice(words).strip()}, '\n",
        "prompt = prompt[:-2]\n",
        "print(f'Prompt: {prompt}')\n",
        "with precision_scope(\"cuda\"):\n",
        "  seed = torch.Generator(\"cuda\").manual_seed(random.randint(0,4294967295))\n",
        "  image = pipe(prompt, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0] \n",
        "  display(image)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q9upICRkMQcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TXT2IMG Method"
      ],
      "metadata": {
        "id": "WcnY9hUsy76f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Huggingface Login\n",
        "from getpass import getpass\n",
        "\n",
        "huggingface_username = '' #@param {type:\"string\"}\n",
        "huggingface_token = '' #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "BKZThsccD4Da",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TXT2IMG Setup\n",
        "import os\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "condacolab.install()\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "import os\n",
        "if not os.path.isdir(root_code):\n",
        "  !mkdir $root_code\n",
        "%cd $root_code\n",
        "!git clone https://github.com/DamascusGit/stable-diffusion.git\n",
        "!mamba env update -n base -f stable-diffusion/environment.yaml\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install kornia==0.6)\n",
        "if not os.path.isdir(root_model):\n",
        "  !mkdir $root_model\n",
        "%cd $root_model\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# Will take a long time\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "!mkdir /content/txt2img_output\n",
        "%cd $code_dir"
      ],
      "metadata": {
        "id": "-HMZ9IiRDs8Q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "torch.cuda.empty_cache()\n",
        "PROMPT = \"matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars\" #@param {type:'string'}\n",
        "STEPS = 160 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 0 #@param {type:'integer'}\n",
        "NUM_ITERS = 6 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "IMAGE_UPSCALER = True #@param {type:\"boolean\"}\n",
        "UPSCALE_AMOUNT = \"2\" #@param [\"2\",\"4\", \"8\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "if IMAGE_UPSCALER:\n",
        "  def upscale(image):\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd /content/Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  import torch\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  model = RealESRGAN(device, scale = int(UPSCALE_AMOUNT))\n",
        "  model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/diffusers_output'\n",
        "epoch_time = int(time.time())\n",
        "if SAVE_PROMPT_DETAILS:\n",
        "  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "with precision_scope(\"cuda\"):\n",
        "  for iteration in range(NUM_ITERS):\n",
        "    \n",
        "    if ORIG_SEED == 0:\n",
        "      rand_num = random.randint(0,4294967295)\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "    else:\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    try:\n",
        "      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    except NameError:\n",
        "      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    \n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "    except NameError:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    print('Upscaling... ')\n",
        "    sr_image = upscale(image)\n",
        "    display(sr_image)\n",
        "    try:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "    except NameError:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "#@markdown `Out of Memory error`: If the VRAM stacks right after execution, sometimes it helps waiting for a minute before running it again. Looking at ways to force it to clear the VRAM"
      ],
      "metadata": {
        "id": "9QnhfmAM0t-X",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to install offline"
      ],
      "metadata": {
        "id": "HQeX4o971T_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://rentry.org/SDInstallation"
      ],
      "metadata": {
        "id": "fFdp-FFg1cao"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OPyJJ2z-RJB7",
        "Z-cd15ZYjLqf",
        "HQeX4o971T_U"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}